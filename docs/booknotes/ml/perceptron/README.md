# 感知机

> [《统计学习方法》 - 李航 - 第二版](https://1drv.ms/b/s!AkcJSyT7tq80f24rxQaaH3HMUWE?e=5vJQNK) 第二章的读书笔记，本文中的所有代码可在[GitHub仓库](https://github.com/LittleBee1024/learning_book/tree/main/docs/booknotes/ml/perceptron/code)中找到

## 理论

感知机(perceptron)是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。

感知机学习旨在求出将训练数据进行线性划分的**分离超平面**，为此，导入基于误分类的**损失函数**，利用梯度下降法对损失函数进行极小化，从而求得感知机模型。

```mermaid
flowchart TB
    subgraph A[学习]
    Data{{训练数据集X,Y}} --利用梯度下降法\n使损失函数最小--> Model[模型: 分离超平面]
    end
```

感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机是神经网络与支持向量机的基础。

```mermaid
flowchart TB
    subgraph B[预测]
    Data{{测试数据X}} --输入--> Model[感知机模型] --根据超平面划分结果--> Res[Y的预测值]
    end
```

### 感知机模型

感知机是根据输入实例的特征向量$x$对其进行二类分类的线性分类模型：

$$
f(x)=\operatorname{sign}(w \cdot x+b)
$$

其中，$w$和$b$为感知机模型参数，$w \cdot x$表示$w$和$x$的内积。sign是符号函数，即

$$
\operatorname{sign}(x) = \begin{cases}+1&x\geq0\\\\-1&x<0\end{cases}
$$

感知机模型对应于输入空间（特征空间）中的分离超平面

$$w \cdot x+b=0$$

其中，$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。如下图所示：

![](./images/perceptron.png)

图中值的证明过程可参考[文章](https://www.jianshu.com/p/c7eddb3ff248)。

### 损失函数

损失函数的一个自然选择是误分点的总数（将误分点个数减少到最小）。但是，这样的损失函数不可导，难以优化。因此我们需要寻找一个更加合适的损失函数，并保证损失函数收敛于误分点最少的点。

感知机学习的损失函数定义为：

$$
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$

其中，$M$是超平面$S$的误分类点集合。通过《统计学习方法》第二章中的推导过程可知，某一误分类点$x_{i}$到超平面$S$的距离是：

$$
-\frac{1}{\left \| w \right \|}y_{i}(w \cdot x_{i}+b)
$$

因此，感知机损失函数的意义是**误分类点**到超平面$S$的总距离。

损失函数$L(w, b)$是非负的。如果没有误分类点，损失函数值是0。误分类点越少，误分类点离超平面越近，损失函数值就越小。利用梯度下降法调整$w$和$b$，以不断极小化损失函数，直到损失函数值为0，此时超平面下没有误分类点。

### 算法

输入：训练数据集$T$，学习率$\eta (0 < \eta \leq 1)$

输出：$w$，$b$，感知机模型$f(x)=\operatorname{sign}(w \cdot x+b)$

利用梯度下降法，求取$w$和$b$的具体过程如下：

1. 选取初值$w_{0}$和$b_{0}$
2. 在训练集中选取数据$(x_{i},y_{i})$
3. 如果$y_{i}(w \cdot x_{i}+b) \leq 0$，表明$(x_{i},y_{i})$是误分类点，需要通过梯度下降法调整$w$和$b$：

$$
\begin{aligned}
&w \leftarrow w + \eta y_{i}x_{i}
\\
&b \leftarrow b + \eta y_{i}
\end{aligned}
$$

4. 跳转至第二步，直到训练集中没有误分类点

上述过程可解释为：当一个实例点被误分类，即位于分离超平面的错误一侧时，调整$w$和$b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。

#### 算法的收敛性

当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。为了得到唯一的超平面，需要对分离超平面增加约束条件，这就是支持向量机的想法来源。

当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。
